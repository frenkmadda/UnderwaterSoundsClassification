{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T14:58:48.936962Z",
     "start_time": "2024-06-28T14:58:46.769637Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import utils \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Attempt to reduce data dimensionality with Structural Similarity Index (SSIM)\n",
    "The entire code is in the file utils.py, it was not reported here because it is refered to an attempt that didn't brought to any result we wanted to keep.\n",
    "The idea was to remove all the images with an SSIM of 0.95 or higher from the classes with more than 500 elements.\n",
    "The results were not as expected, so we decided to keep all the images."
   ],
   "id": "3bc7478038e4f294"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:01:25.622388Z",
     "start_time": "2024-06-22T17:01:25.493168Z"
    }
   },
   "cell_type": "code",
   "source": "utils.create_csv_for_folders('Spettrogrammi/Target/', 8)",
   "id": "5f05cb6ef3ef1ddc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:01:25.675920Z",
     "start_time": "2024-06-22T17:01:25.623385Z"
    }
   },
   "cell_type": "code",
   "source": "utils.create_csv_for_folders('Spettrogrammi/Non-Target/', 8)",
   "id": "af2fa57660b1af71",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "utils.merge_csv_files('Spettrogrammi/Target/dati ssim/SSIM', 'Spettrogrammi/Target/ssim_results.csv')",
   "id": "bef62321cb6b7af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:01:39.797626Z",
     "start_time": "2024-06-22T17:01:39.797558Z"
    }
   },
   "cell_type": "code",
   "source": "utils.merge_csv_files('Spettrogrammi/Non-Target/dati ssim/SSIM', 'Spettrogrammi/Non-Target/ssim_results.csv')",
   "id": "f37899597f9a2b81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_non_target = pd.read_csv('Spettrogrammi/Non-Target/ssim_results.csv')\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "df_non_target.describe()"
   ],
   "id": "ce834f6db4d0fd3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_target = pd.read_csv('Spettrogrammi/Target/ssim_results.csv')\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "df_target.describe()"
   ],
   "id": "a3f75c405ea93b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_non_target = pd.read_csv('Spettrogrammi/Non-Target/df_paths.csv')\n",
    "df_target = pd.read_csv('Spettrogrammi/Target/df_paths.csv')\n",
    "\n",
    "df_non_target.to_csv('final_dataset/df_paths_non_target.csv', index=False)\n",
    "df_target.to_csv('final_dataset/df_paths_target.csv', index=False)"
   ],
   "id": "da620f328499072f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Split Dataset in Train, validation e Test",
   "id": "4933a3fff07a8326"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extracted the numbers of audios for each classes in order to avoid splitting the same audio in the different sets and removed the classes with less than 8 audios.",
   "id": "80ddef7a1495b63d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results of the operations slightly changes if executed on MacOS or Windows because of how different OS handle the files.",
   "id": "4fc577a139dd9dd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:51:43.354766Z",
     "start_time": "2024-06-23T22:51:43.262094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_target = pd.read_csv('Spettrogrammi/Target/df_paths.csv')\n",
    "df_non_target = pd.read_csv('Spettrogrammi/Non-Target/df_paths.csv')"
   ],
   "id": "8deaf93570f90ceb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:51:44.118061Z",
     "start_time": "2024-06-23T22:51:43.980650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "utils.estrai_info_da_csv('Spettrogrammi/Target/df_paths.csv', 'Spettrogrammi/Target/df_class_num.csv')\n",
    "utils.estrai_info_da_csv('Spettrogrammi/Non-Target/df_paths.csv', 'Spettrogrammi/Non-Target/df_class_num.csv')"
   ],
   "id": "74be40225d8d8f13",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:51:45.666605Z",
     "start_time": "2024-06-23T22:51:45.440200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "utils.filter_csv('Spettrogrammi/Target/df_class_num.csv', 'Spettrogrammi/Target/df_paths.csv', 'Spettrogrammi/Target/df_paths_filtered.csv')\n",
    "utils.filter_csv('Spettrogrammi/Non-Target/df_class_num.csv', 'Spettrogrammi/Non-Target/df_paths.csv', 'Spettrogrammi/Non-Target/df_paths_filtered.csv')"
   ],
   "id": "df7a96176616f033",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:51:46.412636Z",
     "start_time": "2024-06-23T22:51:46.401751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_files_into_sets(input_csv_path, train_csv_path, val_csv_path, test_csv_path):\n",
    "    # Imposta il seed per il generatore di numeri casuali\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Carica il file CSV in un DataFrame\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Crea una nuova colonna per il nome del file senza la parte \"_resampled\"\n",
    "    df['File'] = df['FilePath'].apply(lambda x: x.split('_resampled')[0])\n",
    "\n",
    "    # Inizializza una nuova colonna per il set\n",
    "    df['Set'] = ''\n",
    "\n",
    "    # Raggruppa per classe\n",
    "    grouped = df.groupby('Classe')\n",
    "\n",
    "    # Per ogni classe, assegna un set a ciascun file unico\n",
    "    for name, group in grouped:\n",
    "        # Ottieni un elenco di file unici\n",
    "        unique_files = group['File'].unique()\n",
    "\n",
    "        # Calcola le dimensioni dei set\n",
    "        train_size = int(0.8 * len(unique_files))\n",
    "        val_size = int(0.1 * len(unique_files))\n",
    "        test_size = len(unique_files) - train_size - val_size\n",
    "\n",
    "        # Assegna un set a ciascun file unico\n",
    "        file_sets = ['train'] * train_size + ['val'] * val_size + ['test'] * test_size\n",
    "\n",
    "        # Mescola l'elenco dei set di file\n",
    "        np.random.shuffle(file_sets)\n",
    "\n",
    "        # Crea un dizionario che mappa ciascun file al suo set\n",
    "        file_to_set = dict(zip(unique_files, file_sets))\n",
    "\n",
    "        # Assegna ogni riga al set del suo file\n",
    "        df.loc[group.index, 'Set'] = group['File'].apply(lambda x: file_to_set[x])\n",
    "\n",
    "    # Crea tre DataFrame separati per i set di addestramento, validazione e test\n",
    "    df_train = df[df['Set'] == 'train']\n",
    "    df_val = df[df['Set'] == 'val']\n",
    "    df_test = df[df['Set'] == 'test']\n",
    "\n",
    "    # Crea le cartelle di destinazione se non esistono già\n",
    "    os.makedirs(os.path.dirname(train_csv_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(val_csv_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(test_csv_path), exist_ok=True)\n",
    "\n",
    "    # Salva i DataFrame separati come nuovi CSV\n",
    "    df_train.to_csv(train_csv_path, index=False)\n",
    "    df_val.to_csv(val_csv_path, index=False)\n",
    "    df_test.to_csv(test_csv_path, index=False)"
   ],
   "id": "ea654381177c9d9a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:14:06.936988Z",
     "start_time": "2024-06-22T17:14:06.738140Z"
    }
   },
   "cell_type": "code",
   "source": "split_files_into_sets('Spettrogrammi/Target/df_paths_filtered.csv', 'final_dataset/training/df_paths_target_train.csv', 'final_dataset/validation/df_paths_target_val.csv', 'final_dataset/test/df_paths_target_test.csv')",
   "id": "ae9b5f9d34c0df64",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:14:06.980848Z",
     "start_time": "2024-06-22T17:14:06.938185Z"
    }
   },
   "cell_type": "code",
   "source": "split_files_into_sets('Spettrogrammi/Non-Target/df_paths_filtered.csv', 'final_dataset/training/df_paths_non_target_train.csv', 'final_dataset/validation/df_paths_non_target_val.csv', 'final_dataset/test/df_paths_non_target_test.csv')",
   "id": "c344d61e486cbb3b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Distribuzione dei file nel training set",
   "id": "c0033897eda86da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:19:27.158040Z",
     "start_time": "2024-06-24T15:19:27.061655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_train_target = pd.read_csv('final_dataset/training/df_paths_target_train.csv')\n",
    "max_target = np.max(df_train_target['Classe'].value_counts())\n",
    "max_target"
   ],
   "id": "e6f67d7cfd2bf6eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12339"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:19:27.176961Z",
     "start_time": "2024-06-24T15:19:27.159120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_train_non_target = pd.read_csv('final_dataset/training/df_paths_non_target_train.csv')\n",
    "max_non_target = np.max(df_train_non_target['Classe'].value_counts())\n",
    "max_non_target"
   ],
   "id": "efbf6d68b5295ff3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1822"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:19:27.186542Z",
     "start_time": "2024-06-24T15:19:27.178972Z"
    }
   },
   "cell_type": "code",
   "source": "df_train_target['Classe'].value_counts()",
   "id": "808350484cc560ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classe\n",
       "Tanker           12339\n",
       "Passengership    11258\n",
       "Tug              10727\n",
       "Cargo            10169\n",
       "Vessel             170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:19:27.193910Z",
     "start_time": "2024-06-24T15:19:27.188345Z"
    }
   },
   "cell_type": "code",
   "source": "df_train_non_target['Classe'].value_counts()",
   "id": "85c2ec40767e46a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classe\n",
       "Sperm Whale                           1822\n",
       "Humpback Whale                        1657\n",
       "Fin, Finback Whale                     818\n",
       "Soundscape                             239\n",
       "Fraser_s Dolphin                       224\n",
       "Long-Finned Pilot Whale                191\n",
       "Short-Finned (Pacific) Pilot Whale     177\n",
       "Grampus, Risso_s Dolphin               139\n",
       "Killer Whale                           133\n",
       "Spinner Dolphin                        129\n",
       "Striped Dolphin                        128\n",
       "Common Dolphin                         111\n",
       "Bowhead Whale                          107\n",
       "Melon Headed Whale                      85\n",
       "Walrus                                  77\n",
       "Bearded Seal                            75\n",
       "Atlantic Spotted Dolphin                71\n",
       "White-sided Dolphin                     70\n",
       "Harp Seal                               69\n",
       "Ross Seal                               69\n",
       "Clymene Dolphin                         68\n",
       "Pantropical Spotted Dolphin             67\n",
       "Beluga, White Whale                     65\n",
       "Northern Right Whale                    65\n",
       "White-beaked Dolphin                    57\n",
       "False Killer Whale                      55\n",
       "Odontocete                              47\n",
       "Narwhal                                 43\n",
       "Minke Whale                             43\n",
       "Rough-Toothed Dolphin                   40\n",
       "Southern Right Whale                    27\n",
       "Leopard Seal                            26\n",
       "Bottlenose Dolphin                      22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Augmentation",
   "id": "3a39ffce09c9e0fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:19:34.309519Z",
     "start_time": "2024-06-24T15:19:34.288568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_spectrogram(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    spectrogram = np.array(image) / 255 # Normalizza i valori a 0-1\n",
    "    return spectrogram, image.mode\n",
    "\n",
    "def time_shift_spectrogram(image_path, output_path, i, duration = 3):\n",
    "    \"\"\"\n",
    "    Perform a time shift on a spectrogram image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input spectrogram image.\n",
    "    - duration (int): Duration of the audio in seconds.\n",
    "    - shift_seconds (int): Number of seconds to shift.\n",
    "    - output_path (str): Path to save the shifted spectrogram image.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert the image to a NumPy array, preserving all four channels (RGBA)\n",
    "    image_array = np.array(image, dtype=np.uint8)\n",
    "    \n",
    "    shift_seconds = [0.08, 0.16, 0.24, 0.32, 0.40, 0.48, 0.56, 0.64, 0.72, \n",
    "                     0.80, 0.88, 0.96, 1.04, 1.12, 1.20, 1.28, 1.36, 1.44, 1.52]\n",
    "    \n",
    "    # Calculate the number of pixels corresponding to the shift_seconds\n",
    "    width = image_array.shape[1]\n",
    "    pixels_per_second = width // duration\n",
    "    shift_pixels = int(pixels_per_second * shift_seconds[i])\n",
    "    \n",
    "    # Create a new array to hold the time-shifted spectrogram\n",
    "    shifted_image_array = np.zeros_like(image_array, dtype=np.uint8)\n",
    "    \n",
    "    # Shift the image by the calculated number of pixels to the right\n",
    "    shifted_image_array[:, shift_pixels:] = image_array[:, :-shift_pixels]\n",
    "    \n",
    "    # Create a \"silence\" array for the shifted part filled with black pixels\n",
    "    silence = np.zeros((image_array.shape[0], shift_pixels, image_array.shape[2]), dtype=np.uint8)\n",
    "    silence[:, :, 3] = 255  # Set the alpha channel to 255 for full opacity\n",
    "    \n",
    "    # Apply the silence to the beginning of the shifted image\n",
    "    shifted_image_array[:, :shift_pixels] = silence\n",
    "    \n",
    "    # Convert the shifted array back to an image\n",
    "    shifted_image = Image.fromarray(shifted_image_array)\n",
    "    \n",
    "    # Save the shifted image\n",
    "    shifted_image.save(output_path)\n",
    "    \n",
    "    \n",
    "def add_white_noise(spectrogram, mode, save_path, i):\n",
    "    noise_factor = [0.0011, 0.0012, 0.0013, 0.0014, 0.0015, 0.0016, 0.0017, 0.0018, 0.0019, 0.0020, \n",
    "                    0.0021, 0.0022, 0.0023, 0.0024, 0.0025, 0.0026, 0.0027, 0.0028, 0.0029]\n",
    "    \n",
    "    # for noise_factor in noise_factor:\n",
    "    noise = np.random.randn(*spectrogram.shape) * noise_factor[i] # Modificare sta riga se necessario per avere più opzioni di rumore (ad esempio fare /2, ecc....)\n",
    "    augmented_spectrogram = spectrogram + noise\n",
    "    np.clip(augmented_spectrogram, 0, 1)  # Clippa i valori tra 0 e 1\n",
    "    \n",
    "    augmented_spectrogram = (augmented_spectrogram * 255).astype(np.uint8)  # Converti i valori a 0-255\n",
    "    image = Image.fromarray(augmented_spectrogram, mode)  # Converti in immagine mantenendo il mode originale\n",
    "    image.save(save_path)\n",
    "\n",
    "# def augment_spectrogram(image_path, save_dir, width_spectogram=930, shift_seconds=1.0):\n",
    "#     \n",
    "#     # Carica lo spettrogramma originale\n",
    "#     spectrogram, mode = load_spectrogram(image_path)\n",
    "# \n",
    "#     # Calcola la quantità di shift in pixel\n",
    "#     shift_pixels = int(shift_seconds * width_spectogram/3)\n",
    "# \n",
    "#     # Applica il time-shift\n",
    "#     # shifted_spectrogram = time_shift_spectrogram(spectrogram, shift_pixels)\n",
    "#     shifted_image_path = os.path.join(save_dir, f'shifted_{os.path.basename(image_path)}')\n",
    "#     time_shift_spectrogram(image_path, shifted_image_path)\n",
    "#     \n",
    "# \n",
    "#     # Crea la directory se non esiste\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "# \n",
    "#     # Salva lo spettrogramma shiftato\n",
    "#     # save_spectrogram(shifted_spectrogram, shifted_image_path, mode)\n",
    "# \n",
    "#     # Applica il white noise\n",
    "#     noisy_image_path = os.path.join(save_dir, f'noisy_{os.path.basename(image_path)}')\n",
    "#     add_white_noise(spectrogram, mode, noisy_image_path)\n",
    "# \n",
    "# \n",
    "#     # Salva lo spettrogramma con il rumore\n",
    "#     # save_spectrogram(noisy_spectrogram, noisy_image_path, mode)\n",
    "# \n",
    "#     return shifted_image_path, noisy_image_path\n",
    "\n",
    "def time_shift(image_path, save_dir, i):\n",
    "    # shift_seconds = [1.0, 0.5, 0.75, 0.25][np.random.randint(1)]\n",
    "    # Carica lo spettrogramma originale\n",
    "    # spectrogram, mode = load_spectrogram(image_path)\n",
    "\n",
    "    # Calcola la quantità di shift in pixel\n",
    "    # shift_pixels = int(shift_seconds * width_spectogram / 3)\n",
    "\n",
    "    # Applica il time-shift\n",
    "    # shifted_spectrogram = time_shift_spectrogram(spectrogram, shift_pixels)\n",
    "    shifted_image_path = os.path.join(save_dir, f'shifted_{i}_{os.path.basename(image_path)}')\n",
    "    \n",
    "    if not os.path.exists(shifted_image_path):\n",
    "        time_shift_spectrogram(image_path, shifted_image_path, i)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        return shifted_image_path, True\n",
    "    else:\n",
    "        return \"\",False\n",
    "    # Salva lo spettrogramma shiftato\n",
    "    # save_spectrogram(shifted_spectrogram, shifted_image_path, mode)\n",
    "\n",
    "\n",
    "def white_noise(image_path, save_dir, i):\n",
    "    # Carica lo spettrogramma originale\n",
    "    spectrogram, mode = load_spectrogram(image_path)\n",
    "    \n",
    "    # Applica il white noise\n",
    "    noisy_image_path = os.path.join(save_dir, f'noisy_{i}_{os.path.basename(image_path)}')\n",
    "    \n",
    "    if not os.path.exists(noisy_image_path):\n",
    "        add_white_noise(spectrogram, mode, noisy_image_path, i)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "def increase_and_decrease_volume(spectrogram, save_path, mode, i, type):\n",
    "    if type == 'i':\n",
    "        volume_factor = [1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.4, 1.45, 1.5, \n",
    "                         1.55, 1.6, 1.65, 1.7, 1.75, 1.8, 1.85, 1.9, 1.95, 2]\n",
    "    else: # type == 'd'\n",
    "        volume_factor = [0.15, 0.175, 0.19, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, \n",
    "                         0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "        \n",
    "    # Aumenta il volume solo sui canali RGB, lasciando il canale alpha invariato\n",
    "    spectrogram[..., :3] *= volume_factor[i]\n",
    "\n",
    "    # Clippa i valori tra 0 e 255\n",
    "    spectrogram = np.clip(spectrogram, 0, 255)\n",
    "\n",
    "    # Salva lo spettrogramma modificato\n",
    "    amplified_image = Image.fromarray(spectrogram.astype(np.uint8), mode)\n",
    "    amplified_image.save(save_path)\n",
    "    \n",
    "def increase_and_decrease(image_path, save_dir, i, type):\n",
    "    image = Image.open(image_path)\n",
    "    spectrogram = np.array(image).astype(np.float32)\n",
    "    mode = image.mode\n",
    "    \n",
    "    if type == 'i':\n",
    "        increase_image_path = os.path.join(save_dir, f'increase_{i}_{os.path.basename(image_path)}')\n",
    "    else: # type == 'd'\n",
    "        increase_image_path = os.path.join(save_dir, f'decrease_{i}_{os.path.basename(image_path)}')\n",
    "    \n",
    "    if os.path.exists(increase_image_path):\n",
    "        return False\n",
    "    else:\n",
    "        increase_and_decrease_volume(spectrogram, increase_image_path, mode, i, type)\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        return True\n",
    "\n",
    "def process_csv(csv_path, save_dir, value_counts):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    max = np.max(value_counts)\n",
    "    i = 0\n",
    "\n",
    "    while np.any(value_counts < max):\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing images\"):\n",
    "            image_path = row.iloc[0]\n",
    "\n",
    "            parts = image_path.split('/')\n",
    "            relevant_parts = parts[1:-1]\n",
    "\n",
    "            destination_dir = os.path.join(save_dir, *relevant_parts[:-1])\n",
    "            destination_path = os.path.join(destination_dir, relevant_parts[-1])\n",
    "\n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "            # If the curent class has reached the maximum, go to the next iteration\n",
    "            if value_counts.loc[parts[2]] >= max:\n",
    "                continue\n",
    "\n",
    "            shifted_path, done_f = time_shift(image_path, destination_path, i)\n",
    "            if done_f:\n",
    "                value_counts.loc[parts[2]] += 1\n",
    "            \n",
    "            #Portion of the code to merge time shift and increase and decrease, done_f is a flag that indicates if the time shift has been done to then also do the increase and decrease, if there was no done_f in case of duplicate, the function would crash\n",
    "            if value_counts.loc[parts[2]] < max and done_f:\n",
    "                 done = increase_and_decrease(shifted_path, destination_path, i, 'i')\n",
    "                 if done:\n",
    "                    value_counts.loc[parts[2]] += 1\n",
    "                 \n",
    "            #if value_counts.loc[parts[2]] < max and done_f:\n",
    "            #    done = increase_and_decrease(shifted_path, destination_path, i, 'd')\n",
    "            #    if done:\n",
    "            #        value_counts.loc[parts[2]] += 1\n",
    "            #----------------\n",
    "            if value_counts.loc[parts[2]] < max:\n",
    "               done = white_noise(image_path, destination_path, i)\n",
    "               if done:\n",
    "                    value_counts.loc[parts[2]] += 1\n",
    "\n",
    "            if value_counts.loc[parts[2]] < max:\n",
    "                done = increase_and_decrease(image_path, destination_path, i, 'i')\n",
    "                if done:\n",
    "                    value_counts.loc[parts[2]] += 1\n",
    "\n",
    "            if value_counts.loc[parts[2]] < max:\n",
    "                done = increase_and_decrease(image_path, destination_path, i, 'd')\n",
    "                if done:\n",
    "                    value_counts.loc[parts[2]] += 1\n",
    "\n",
    "            i += 1\n",
    "            if i == 19:\n",
    "                i = 0"
   ],
   "id": "56c856a780af0575",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:52:31.808007Z",
     "start_time": "2024-06-24T15:19:46.053955Z"
    }
   },
   "cell_type": "code",
   "source": "process_csv('final_dataset/training/df_paths_target_train.csv', 'final_dataset/training', df_train_target['Classe'].value_counts())",
   "id": "85b7192789ea814f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 44663/44663 [10:17<00:00, 72.31it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:33<00:00, 477.31it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:33<00:00, 475.43it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:33<00:00, 476.48it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:36<00:00, 464.65it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:42<00:00, 434.57it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:39<00:00, 449.57it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:33<00:00, 479.05it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:39<00:00, 450.77it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:44<00:00, 425.69it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:48<00:00, 411.13it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:49<00:00, 407.58it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:49<00:00, 408.62it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [01:47<00:00, 415.73it/s]  \n",
      "Processing images: 100%|██████████| 44663/44663 [00:36<00:00, 1230.89it/s] \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- bilanciare\n",
    "- pregare che funzioni"
   ],
   "id": "c5f7eeb2e0760605"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:28:36.772863Z",
     "start_time": "2024-06-24T16:28:36.765760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_rows_to_csv(root_dir, csv_path, new_csv_path):\n",
    "    # Carica il CSV esistente in un DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Scorri tutte le sottocartelle e i file .png\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.png'):\n",
    "                # Crea il percorso completo del file\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Estrai il nome della classe dalla sottocartella\n",
    "                class_name = os.path.basename(dirpath)\n",
    "\n",
    "                # Costruisci il nome del file rimuovendo la parte '_resampled'\n",
    "                file_name = full_path.split('_resampled')[0]\n",
    "\n",
    "                # Crea una nuova riga\n",
    "                new_row = pd.DataFrame({'FilePath': [full_path], 'Classe': [class_name], 'File': [file_name], 'Set': ['train']})\n",
    "\n",
    "                # Aggiungi la nuova riga al DataFrame\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Salva il DataFrame modificato in un nuovo CSV\n",
    "    df.to_csv(new_csv_path, index=False)"
   ],
   "id": "a5ee740d48439065",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:28:59.016107Z",
     "start_time": "2024-06-24T16:28:38.882331Z"
    }
   },
   "cell_type": "code",
   "source": "add_rows_to_csv('final_dataset/training/Target', 'final_dataset/training/df_paths_target_train.csv', 'final_dataset/training/new_df_paths_target_train.csv')",
   "id": "99eee970c454916c",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:29:52.610264Z",
     "start_time": "2024-06-24T16:28:59.017407Z"
    }
   },
   "cell_type": "code",
   "source": "add_rows_to_csv('final_dataset/training/Non-Target', 'final_dataset/training/df_paths_non_target_train.csv', 'final_dataset/training/new_df_paths_non_target_train.csv')",
   "id": "328e73df722ea7f8",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:59:19.856904Z",
     "start_time": "2024-06-28T14:59:19.845512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_csv_files(csv1_path, csv2_path, output_path):\n",
    "    # Carica i due CSV in due DataFrame separati\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "\n",
    "    # Unisci i due DataFrame\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Crea la colonna \"Label\" in base alla presenza di \"Non-Target\" o \"Target\" in FilePath\n",
    "    df['Label'] = df['FilePath'].apply(lambda x: 'Non-Target' if \"Non-Target\" in x else 'Target')\n",
    "\n",
    "    # Mescola le righe del DataFrame\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Salva il DataFrame modificato in un nuovo CSV\n",
    "    df.to_csv(output_path, index=False)"
   ],
   "id": "bb9e8a4b788e2d34",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:04:37.694911Z",
     "start_time": "2024-06-24T17:04:36.972284Z"
    }
   },
   "cell_type": "code",
   "source": "merge_csv_files('final_dataset/training/new_df_paths_target_train.csv', 'final_dataset/training/new_df_paths_non_target_train.csv', 'final_dataset/training/df_paths_train.csv')",
   "id": "820cdc09a254a5a8",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:09:35.215188Z",
     "start_time": "2024-06-24T17:09:35.209658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_csv_merge(csv_path, save_dir, value_counts):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    max = np.max(value_counts)\n",
    "    i = 0\n",
    "\n",
    "    while np.any(value_counts < max):\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing images\"):\n",
    "            image_path = row.iloc[0]\n",
    "\n",
    "            parts = image_path.split('/')\n",
    "            if 'Spettrogrammi' in parts[0]:\n",
    "                relevant_parts = parts[1:-1]\n",
    "                label = parts[1]\n",
    "            else:\n",
    "                relevant_parts = parts[2:-1]\n",
    "                label = parts[2]\n",
    "\n",
    "            destination_dir = os.path.join(save_dir, *relevant_parts[:-1])\n",
    "            destination_path = os.path.join(destination_dir, relevant_parts[-1])\n",
    "            \n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "            # If the curent class has reached the maximum, go to the next iteration\n",
    "            if value_counts.loc[label] >= max:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if value_counts.loc[label] < max and \"shifted\" in image_path and \"increase\" not in image_path:\n",
    "                done = increase_and_decrease(image_path, destination_path, i, 'd')\n",
    "                if done:\n",
    "                    value_counts.loc[parts[2]] += 1\n",
    "\n",
    "\n",
    "            i += 1\n",
    "            if i == 19:\n",
    "                i = 0"
   ],
   "id": "f905eec7cd164c17",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 121821/121821 [01:21<00:00, 1495.76it/s] \n"
     ]
    }
   ],
   "execution_count": 88,
   "source": "process_csv_merge('final_dataset/training/df_paths_train.csv', 'final_dataset/training/', df_train_target['Label'].value_counts())",
   "id": "a8fd9b27f0ec4778"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:15:35.875875Z",
     "start_time": "2024-06-24T17:14:53.912452Z"
    }
   },
   "cell_type": "code",
   "source": "add_rows_to_csv('final_dataset/training/Non-Target', 'final_dataset/training/df_paths_non_target_train.csv', 'final_dataset/training/new_df_paths_non_target_train.csv')",
   "id": "162923608478f806",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:16:37.534017Z",
     "start_time": "2024-06-24T17:16:37.022028Z"
    }
   },
   "cell_type": "code",
   "source": "merge_csv_files('final_dataset/training/new_df_paths_target_train.csv', 'final_dataset/training/new_df_paths_non_target_train.csv', 'final_dataset/training/df_paths_train.csv')",
   "id": "52ccf97b4f9e8d9d",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:16:42.623448Z",
     "start_time": "2024-06-24T17:16:42.416017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_train_target = pd.read_csv('final_dataset/training/df_paths_train.csv')\n",
    "# df_train_target\n",
    "max_target = np.max(df_train_target['Label'].value_counts())\n",
    "max_target"
   ],
   "id": "925e03861f1d952e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61695"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:16:45.125966Z",
     "start_time": "2024-06-24T17:16:45.119599Z"
    }
   },
   "cell_type": "code",
   "source": "df_train_target['Label'].value_counts()",
   "id": "8d4e61d03abca36d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non-Target    61695\n",
       "Target        61695\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T13:27:55.978073Z",
     "start_time": "2024-06-26T13:27:55.928731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Unione file csv per il validation set\n",
    "merge_csv_files('final_dataset/validation/df_paths_target_val.csv', 'final_dataset/validation/df_paths_non_target_val.csv', 'final_dataset/validation/df_paths_val.csv')"
   ],
   "id": "7eeffe999a3f9ceb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:57:43.755156Z",
     "start_time": "2024-06-26T14:57:43.541523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# conta elementi png in una cartella e sottocartelle\n",
    "def count_files_in_folder(folder_path):\n",
    "    count = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.png'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "count_files_in_folder('/Users/daniela/Library/CloudStorage/OneDrive-UniversitàdiSalerno/I Anno/Secondo Semestre/Fondamenti di Visione Artificiale e Biometria/Progetto/biometria/final_dataset/Non-Target')"
   ],
   "id": "91b4ba2ad3ff52c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54679"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:59:27.316025Z",
     "start_time": "2024-06-28T14:59:26.438257Z"
    }
   },
   "cell_type": "code",
   "source": "merge_csv_files('final_dataset/training/df_paths_target_train.csv', 'final_dataset/training/df_paths_non_target_train.csv', 'final_dataset/training/df_paths_train_clean.csv')",
   "id": "4de4a351c795a1b5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a9bb587005cd6290"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
